% Based on template for ICASSP-2010 paper; to be used with:
%          02456.sty  - 02456 LaTeX style file adapted from ICASSP
\documentclass{article}
\usepackage{amsmath,graphicx,02456}
\usepackage{booktabs}
\usepackage{subcaption}
\raggedbottom
% For making citations and URLs blue and clickable
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{float}
\usepackage{xurl}
% ---- Compact layout tweaks ----
% \usepackage[margin=1.5cm]{geometry}      % smaller margins (if allowed)
% \setlength{\parskip}{0pt}                % no extra space between paragraphs
% \setlength{\parindent}{0.5em}            % small paragraph indent

% \usepackage{enumitem}                    % compact lists
% \setlist{nosep}                          % no extra space in itemize/enumerate

% \usepackage{titlesec}                    % tighter section spacing
% \titlespacing*{\section}{0pt}{*0.8}{*0.5}
% \titlespacing*{\subsection}{0pt}{*0.7}{*0.4}
% \usepackage[font=small,labelfont=bf]{caption} % smaller captions
% \usepackage{geometry}
% \geometry{
%   letterpaper,
%   textwidth=7in,
%   textheight=9in,
%   left=0.75in,
%   top=1in,
%   includefoot=false  % Adjust if footers are needed
% }



\toappear{02456 Deep Learning, DTU Compute, Fall 2025}

% Title.
% ------
\title{Semi-supervised learning for drug discovery}

% Author names and student numbers
% --------------------------------
\name{%
  \begin{tabular}{c}
    Authors here...
  \end{tabular}
}
\address{}

\begin{document}

\maketitle

\begin{abstract}
Molecular property prediction is essential in drug discovery, yet labeled data remains scarce and expensive. Graph Neural Networks (GNNs) provide a natural framework for learning from molecular structures, but their performance degrades in low-label regimes. We investigate whether semi-supervised learning can improve GNN-based property prediction by leveraging unlabeled molecular data. Using the QM9 dataset, we compare several GNN architectures and find that geometric models encoding 3D information, particularly DimeNet++, outperform approaches such as GCN and GIN. We then evaluate three semi-supervised strategies: Mean Teacher, a simplified $n$-network consistency scheme inspired by Cross Pseudo Supervision, and consistency regularization with graph augmentations. Mean Teacher yields a modest but consistent improvement, reducing validation MSE by approximately 5\% out of the box and 25\% with optimized parameters. However, gains remain limited, suggesting that expressive geometric GNNs already extract most available signal from the labeled subset. We discuss implications for semi-supervised molecular learning and directions for future work.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Drug discovery depends on predicting molecular properties from structure, yet labeled molecular data is scarce and expensive to obtain. Meanwhile, large collections of unlabeled molecules are readily available. Graph Neural Networks (GNNs) are well suited for this task, as molecules naturally represent as graphs with atoms as nodes and bonds as edges. However, GNN performance degrades in low-label regimes, motivating the use of semi-supervised learning to leverage unlabeled data.

To systematically investigate this, we follow a three-stage experimental protocol. In Stage 1, we compare several GNN architectures under fully supervised training to identify the strongest backbone for molecular property prediction. In Stage 2, we fix this backbone and evaluate three semi-supervised learning strategies to determine which best exploits unlabeled data. In Stage 3, we focus on the most promising strategy and refine it through targeted hyperparameter tuning and regularization. This staged approach allows us to isolate the contributions of architecture choice, the semi-supervised method, and optimization separately, providing clear insights into what drives performance in low-label molecular learning.

\section{Problem Formulation \& Background}
\subsection{Deep learning for molecular graphs}
Molecules naturally lend themselves to graph representations, making GNNs a fitting choice since they operate directly on graph-structured data. Most GNN architectures are based on message passing. At each layer, every node updates its representation by aggregating information from its neighbors. This process typically consists of the following steps. First the message function computes the information each node sends to its neighbors, based on node features and edge features and then the aggregation function combines all incoming messages for a node using a permutation-invariant operation such as sum, mean, or max.

\section{Dataset}
We use the QM9 dataset~\cite{pyg_qm9_docs}, comprising $\approx$130k molecules with 3D geometries represented as graphs, to predict the HOMO energy~\cite{homo}. The data is divided into a fixed random split of $72\%/8\%/10\%/10\%$ (unlabeled/labeled/val/test). To ensure stability without data leakage, targets are normalized using the mean and standard deviation of the labeled training set only, and predictions are unscaled to $eV$ for evaluation.


% Her skal vi skrive hvilke modeller vi bruger og hvad der gør dem specielle
\section{Description of models and methods}
In this section, we briefly describe each model in our evaluation, along with the semi-supervised methods we use. The section focuses on the core mechanisms and the motivation for including each model.

\subsection{Models} 
The models used are message-passing GNNs. We represent each molecule as a graph $G = (V, E)$ with atoms $i \in V$, bonds $(i,j) \in E$ and initial node features $h_i^{(0)}$ (e.g. atom type) and edge features $e_{ij}$ (e.g. distance). A generic message-passing layer updates node representations as~\cite{gilmer2017neural}:
\begin{align}
    m_{ij}^{(\ell)} &= \psi^{(\ell)}\!\big(h_i^{(\ell)}, h_j^{(\ell)}, e_{ij}\big), \\
    m_{i}^{(\ell)}  &= \mathrm{AGG}_{j \in \mathcal{N}(i)} m_{ij}^{(\ell)}, \\
    h_i^{(\ell+1)} &= \phi^{(\ell)}\!\big(h_i^{(\ell)}, m_i^{(\ell)}\big),
\end{align}
It follows the standard message-passing GNN formulation used in prior work on molecular graphs, where:

\begin{itemize}
    \item $\mathcal{N}(i)$ is the set of neighbors of atom $i$
    \item $\psi^{(\ell)}$ and $\phi^{(\ell)}$ are learnable functions
    \item $\mathrm{AGG}$ is a permutation-invariant aggregation (e.g., sum or mean)

\end{itemize}
\noindent After several layers, a graph-level representation is obtained by pooling over all atoms and fed into a regression head to predict the HOMO energy.

\textbf{Graph Convolutional Networks (GCN)} follow the standard message-passing scheme with a simple, normalized aggregation where each node updates its representation by averaging features from its neighbors. This makes GCNs efficient and stable, but the uniform weighting also limits their ability to distinguish subtle structural differences~\cite{GCN}.

\textbf{Graph Isomorphism Networks (GIN)} builds on the same message-passing idea but replaces the normalized average with a sum aggregation and a more expressive MLP update function $\phi^{(\ell)}$. This design overcomes GCN’s under-expressiveness and allows GIN to better differentiate structurally similar yet chemically distinct molecular graphs~\cite{GIN}.
% \textbf{AttentiveFP} replaces the simple aggregation with an attention mechanism that learns to weigh neighbor contributions differently. It also applies a second attention layer during graph-level readout, enabling the model to focus on the most informative atoms when producing the final molecular representation. This two-level attention makes it particularly suited for molecular property prediction where certain functional groups or substructures dominate the target property.\cite{AFP}

\textbf{SchNet} is a continuous-filter convolutional network that uses radial basis functions of inter-atomic distances as edge features $e_{ij}$, allowing it to operate directly on 3D coordinates~\cite{SchNet}.

% \textbf{VisNet} is an equivariant GNN that encodes not only distances
% but also directions between atoms using geometric vectors as part of
% the message function $\psi^{(\ell)}$, making it well-suited for 3D
% molecular geometry~\cite{VisNet}.
\textbf{DimeNet++} uses directional message passing where $e_{ij}$ includes both distances and bond angles via spherical basis functions.
This allows the model to capture higher-order geometric patterns and has been shown to work well on molecular benchmarks~\cite{DimeNet++}.

\subsection{Semi-supervised learning methods}\label{sec:ssl-methods}
% Describe the SSL methods we have used, why we chose them and what they may be good for
We explored three different semi-supervised learning strategies to make use of the large quantity of unlabeled molecular data. Each method introduces a different form of consistency constraint to try and improve model generalization.
Let $\mathcal{D}_L = \{(x_i, y_i)\}$ be the labeled set and
$\mathcal{D}_U = \{x_j\}$ the unlabeled set. For a model $f_\theta$,
we minimize

\begin{equation}
\small
    \mathcal{L}(\theta, t)
    =
    \underbrace{\frac{1}{|\mathcal{D}_L|}
    \sum_{(x,y)\in \mathcal{D}_L}
    \ell\big(f_\theta(x), y\big)}_{\mathcal{L}_{\text{sup}}}
    +
    \lambda(t)\,
    \underbrace{\frac{1}{|\mathcal{D}_U|}
    \sum_{x\in \mathcal{D}_U}
    \mathcal{L}_{\text{cons}}(x)}_{\mathcal{L}_{\text{cons}}},
    \label{eq:ssl-objective}
\end{equation}

\noindent where $\ell$ is the supervised regression loss (MSE in our case), $\mathcal{L}_{\text{cons}}$ measures prediction consistency on unlabeled molecules, and $\lambda(t)$ is a ramp-up schedule that gradually increases the strength of the consistency term during training.

The first method is the mean teacher approach. This method maintains two copies of the model: a student and a teacher. The student is updated by gradient descent, while the teacher parameters are an exponential moving average of the student parameters. For unlabeled molecules, we apply a consistency loss between the student prediction and the teacher prediction. This encourages stable predictions around unlabeled samples and uses them as an additional regularizer during training~\cite{mean-teacher}.

% The second method is n--CPS. Cross Pseudo Supervision (CPS) trains two segmentation networks that generate pseudo-labels for eachother on unlabeled data~\cite{cps}. n--CPS extends this idea to $n$ networks that learn jointly from eachother's predictions through consistency regularization~\cite{n--cps}. In our adaptation to molecular property regression, we train multiple GNNs in paralel and penalise disagreement between their predictions on unlabeled molecules, while still using the ground-truth labels on the labeled subset. A ramp-up schedule controls the strength of this cross-supervision term to avoid unstable training in the early epochs.
% Fandt ud af at jeg/chatten ikke har lavet n--cps helt efter bogen, men en simplificeret version:

% The second method is a simplified $n$-network consistency scheme inspired by Cross Pseudo Supervision (s-n-CPS)~\cite{cps,n-cps}. We train multiple GNNs in parallel with a shared optimizer. For labeled molecules, we average the supervised loss (MSE) across networks. For unlabeled molecules, each network's prediction is pushed toward the mean prediction of the other networks via a consistency loss, with a ramp-up schedule controlling its weight during training. Unlike full n-CPS, we omit data augmentation and uncertainty-based filtering to isolate the effect of cross-network consistency from our augmentation-based method.

The second method is a simplified $n$-network consistency scheme inspired by Cross Pseudo Supervision (CPS) and its $n$-network extension (n-CPS)~\cite{cps, n-cps}. We use the simplified variant rather than full n-CPS to better isolate cross-network consistency and avoid overlap with our augmentation-based consistency method, hence the name s-n-CPS.

In our adaptation to molecular property regression, we train multiple GNNs in parallel and share a single optimizer and scheduler across all model parameters. For labeled molecules we compute the MSE, and for an unlabeled batch every network processes the same molecules. We construct a soft pseudo-target as the mean prediction of the other networks in the ensemble. A ramp-up schedule controls the weight of this consistency term, gradually increasing its influence to avoid unstable training in the early epochs. Unlike the original CPS/n-CPS setups, we do not use data augmentation or uncertainty-based filtering of pseudo-labels, and we primarily rely on soft pseudo-targets rather than hard class labels.

The third method uses consistency regularization with graph data augmentation. Consistency regularization encourages a model to produce similar outputs for different versions of the same input~\cite{zhang-consistency}. We follow consistency-regularized GNN approaches for molecular property prediction~\cite{consistency-gnn} and implement this idea by applying graph-level augmentations to the molecular data - we add Gaussian noise to the atomic coordinates and randomly mask node features. The GNN processes both the original and the augmented graphs and we add a consistency loss that penalizes differences between their predictions - especially on unlabeled molecules. This is meant to push the model towards smooth decision boundaries in meaningful regions of input space and allow it to exploit unlabeled data without explicit pseudo-labels.

% \section{Experiments \& Evaluation}

% To empirically validate our semi-supervised learning framework, we designed a three-stage experimental protocol. All experiments were conducted using High-Performance Computing (HPC) array jobs to ensure parallel execution and resource isolation. To facilitate rigorous analysis and reproducibility, experiment artifacts and metrics were logged to Weights \& Biases (W\&B). For the analysis presented below, run histories were aggregated via the W\&B Python API into Pandas DataFrames, allowing for the systematic generation of performance visualizations. All hyperparameters and design choices (architecture, learning rate, weight decay, consistency weight, etc.) are selected based solely on performance on the validation split. The test split remains untouched during development and is evaluated exactly once for each final model configuration chosen based on validation performance.

% \subsection{Stage 1: Architecture Selection and Hyperparameter Search}


% \textbf{Preliminary Exploration}
% \\
% Prior to performing the structured grid search, we conducted a series of exploratory experiments to identify sensible baselines for model depth, optimizers, and learning rate schedules. 
% \\
% During this phase, we observed distinct behaviors across architecture types. For example, when increasing the depth of simple Message Passing Neural Networks (MPNNs) like GCN beyond 4-5 layers, we observed performance degradation rather than improvement. This is likely attributable to the over-smoothing phenomenon, where node representations become indistinguishable in deeper networks. Conversely, geometric models like SchNet required careful initialization and smaller learning rates to avoid exploding gradients during early training steps. We also found that a static learning rate led to plateauing validation loss; consequently, we integrated a \texttt{ReduceLROnPlateau} scheduler, which we observed significantly improved convergence stability in the final epochs.
% \\\\
% \textbf{System Optimization \& Infrastructure}
% \\
% Prior to commencing the large-scale sweeps, we performed a profiling analysis of the training pipeline on the HPC cluster. We identified a significant bottleneck in the CPU-to-GPU data transfer and the dataloader worker instantiation. By optimizing the \texttt{num\_workers} parameter relative to the cluster's CPU core availability and increasing the inference batch size to saturate the GPU memory, we achieved a \textbf{5x reduction in wall-clock training time} per epoch. This optimization was critical in allowing us to extend the training duration and perform the extensive hyperparameter sweeps described in Stage 3.
% \\\\
% \textbf{Structured Search Protocol}
% \\
% Building on these insights, we executed a full grid search to select the optimal backbone architecture for the subsequent semi-supervised learning stage. We evaluated four distinct architectures: \textbf{GCN}, \textbf{GIN}, \textbf{SchNet}, and \textbf{DimeNet++}.

% We employed a fully supervised training regime on the QM9 dataset, sweeping over a hyperparameter grid consisting of two learning rates ($5\text{e-}4$ and $1\text{e-}3$) and three weight decay values ($0$, $1\text{e-}4$, and $5\text{e-}4$). This resulted in a total of $4 \text{ models} \times 2 \text{LRs} \times 3 \text{WDs} = 24$ distinct experimental configurations.
% \\\\
% \textbf{Results and Analysis}
% \\
% The results of the architecture search are summarized in Figure \ref{fig:stage1_boxplot}. We found that \textbf{DimeNet++} consistently outperformed the other architectures, achieving the lowest median Validation MSE and demonstrating high robustness to hyperparameter variations. 

% \begin{figure}[H]
%     \centering
%     % --- First Subfigure ---
%     \begin{subfigure}[b]{0.225\textwidth}
%         \centering
%         % Width is set to linewidth relative to this subfigure
%         \includegraphics[width=\linewidth]{stage1_boxplot.png}
%         \caption{\textbf{Architecture Performance Distribution.}}
%         \label{fig:stage1_boxplot}
%     \end{subfigure}
%     \hfill % Adds space between the two images
%     % --- Second Subfigure ---
%     \begin{subfigure}[b]{0.225\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{stage1_learning_curves.png}
%         \caption{\textbf{Training Convergence Analysis.}}
%         \label{fig:stage1_curves}
%     \end{subfigure}
    
%     % --- Main Caption and Label ---
%     \caption{\textbf{Stage 1 Results.} (a) Box plots aggregating the best validation MSE. DimeNet++ demonstrates superior predictive performance. (b) Validation MSE curves over 150 epochs. DimeNet++ shows the most consistent convergence.}
%     \label{fig:stage1_combined}
% \end{figure}

% % \begin{figure}[H]
% %     \centering
% %     % Replace with your actual filename
% %     \includegraphics[width=0.9\linewidth]{stage1_boxplot.png}
% %     \caption{\textbf{Architecture Performance Distribution.} Box plots aggregating the best validation MSE across all hyperparameter combinations (LR and Weight Decay) for each architecture. DimeNet++ demonstrates superior predictive performance compared to GCN, GIN, and SchNet.}
% %     \label{fig:stage1_boxplot}
% % \end{figure}

% \noindent While GCN and GIN converged quickly, their error floors remained higher than the geometric approaches (SchNet and DimeNet++). This aligns with the expectation that QM9 targets (quantum chemical properties) are inherently dependent on 3D geometry (bond angles and distances), which standard topological GNNs capture less effectively.

% To verify training stability, we visualized the convergence behavior of the models. Figure \ref{fig:stage1_curves} illustrates the validation loss trajectories. We observed that while higher learning rates ($1\text{e-}3$) led to faster initial descent, they occasionally introduced volatility in the validation metrics for the geometric models.

% % \begin{figure}[h]
% %     \centering
% %     % Replace with your actual filename
% %     \includegraphics[width=\linewidth]{stage1_learning_curves.png}
% %     \caption{\textbf{Training Convergence Analysis.} Validation MSE curves over 150 epochs, faceted by architecture. Colors indicate Learning Rate and line styles indicate Weight Decay. DimeNet++ shows the most consistent convergenkce toward a lower loss minimum.}
% %     \label{fig:stage1_curves}
% % \end{figure}

% \noindent Based on the lowest achieved Validation MSE in this stage, we selected \textbf{DimeNet++} as the fixed backbone architecture for Stage 2. The best-performing hyperparameter combination (LR $= 0.001$, WD $= 0.0005$) was frozen to ensure a fair comparison of the semi-supervised strategies discussed in the following section.

% \subsection{Stage 2: Semi-Supervised Learning Strategy Comparison}

% Having identified DimeNet++ as the optimal backbone architecture in Stage 1, we proceeded to evaluate the efficacy of semi-supervised learning (SSL) strategies. We fixed the hyperparameters to the optimal configuration found in the grid search (LR $= 1\text{e-}3$, WD $= 5\text{e-}4$) to isolate the impact of the SSL methodology.

% We compared four distinct strategies: n-CPS (Cross Pseudo Supervision), Consistency Regularization with geometric augmentations, Mean Teacher, and a fully supervised DimeNet++ baseline (labeled ‘Semi-Supervised Ensemble’ in the plots). All three SSL models were trained using the same split of labeled and unlabeled data to ensure a fair comparison.
% \\\\
% \textbf{Results and Convergence}
% \\
% The validation MSE trajectories for the four strategies are presented in Figure \ref{fig:stage2_curves}. We observed distinct convergence behaviors among the methods:

% % \begin{figure}[h]
% %     \centering
% %     % Upload your screenshot as 'stage2_wandb.png'
% %     \includegraphics[width=0.5\linewidth]{stage2_wandb.png}
% %     \caption{\textbf{Validation MSE Trajectories for Stage 2.} Comparison of three semi-supervised strategies (and fully supervised) using the DimeNet++ backbone. The Mean Teacher approach (magenta line) demonstrates the fastest convergence and lowest final error ($0.0165$), while n-CPS and Consistency Regularization show higher variance during training.}
% %     \label{fig:stage2_curves}
% % \end{figure}

% \begin{figure}[h]
%     \centering
%     % Left side: The Image
%     % [c] aligns the image vertically to the center
%     \begin{minipage}[c]{0.58\linewidth} 
%         \includegraphics[width=\linewidth]{stage2_wandb.png}
%     \end{minipage}
%     \hfill % Adds flexible space between image and text
%     % Right side: The Caption
%     \begin{minipage}[c]{0.38\linewidth}
%         \caption{\textbf{Validation MSE Trajectories for Stage 2.} Comparison of three semi-supervised strategies using DimeNet++. The Mean Teacher approach (magenta) demonstrates the fastest convergence and lowest final error ($0.0165$).}
%         \label{fig:stage2_curves}
%     \end{minipage}
% \end{figure}

% \noindent Mean Teacher (Magenta) demonstrated the most stable convergence profile and achieved the lowest final Validation - $0.0165$. n-CPS (Blue) and Consistency Augmentation (Yellow) exhibited higher volatility in the early epochs, likely due to the stochastic nature of the pseudo-label generation and geometric perturbations, respectively.

% \subsection{Stage 3: Optimization and Regularization of Mean Teacher}

% Having identified Mean Teacher as the optimal strategy, the final stage focused on maximizing its performance through hyperparameter tuning and regularization. We conducted a combinatorial sweep over four key hyperparameters: coordinate noise injection ($\sigma \in \{0.01, 0.02\}$), unsupervised consistency weight ($\lambda \in \{2.0, 5.0\}$), scheduler decay rate ($\gamma \in \{0.98, 0.99\}$), and optimizer weight decay ($\{1\text{e-}4, 5\text{e-}3\}$).
% \\\\
% \textbf{Overfitting \& Scheduling Challenges}
% During the initial sweeps of Stage 3, we observed a distinct overfitting phenomenon. While the supervised training loss approached zero ($\approx 0.0001$), the validation MSE bottomed out early (approx. epoch 100) and subsequently degraded.

% Our analysis identified two primary causes. Firstly, the scheduler conflict. The ReduceLROnPlateau scheduler failed to trigger effectively because the validation interval was synchronized with the patience counter, resulting in a learning rate that remained high for too long. Secondly, model capacity. The standard DimeNet++ configuration (128 hidden channels) possessed sufficient capacity to memorize the labeled set, rendering the consistency loss ineffective as a regularizer.

% To address these issues, we implemented three key adjustments in the final runs. First, we replaced the plateau-based scheduler with CosineAnnealingLR, which forces the learning rate to decay to zero by the end of training. This guaranteed convergence and eliminated the hyperparameter sensitivity we observed with the previous scheduler. Second, we increased regularization by raising the weight decay from $1\text{e-}4$ to $5\text{e-}3$ and setting the unsupervised loss weight to $1.0$. Third, we reduced training to 50 epochs to prevent overfitting while maintaining performance.

% These interventions successfully mitigated the overfitting. The final configuration yielded a healthier training curve where the gap between training and validation loss was significantly reduced, ultimately achieving a Validation MSE of $\mathbf{0.0124}$ and a Test MSE of $\mathbf{0.0134}$, the lowest errors observed across all experimental stages.

% \subsection{Performance Analysis}
% A quantitative comparison reveals that while SSL strategies improved upon the baseline, the gains were incremental rather than transformative.

% \textbf{Baseline Strength:} DimeNet++ is a highly expressive architecture that captures 3D angular information effectively. With the provided amount of labeled data, the model may already be approaching the aleatoric uncertainty limit (the noise floor) of the dataset, leaving little room for unlabeled data to correct decision boundaries.

% \textbf{Mean Teacher Effectiveness:} Despite the saturation, Mean Teacher succeeded where others struggled because it enforces temporal consistency. By averaging model weights over training steps, it acts as a regularizer that prevents the model from overfitting to the noisy pseudo-labels generated in the early stages of SSL.

\section{Experiments \& Evaluation}
\label{sec:experiments}

To evaluate our semi-supervised framework, we designed a three-stage protocol: (i) architecture and hyperparameter selection, (ii) comparison of semi-supervised learning (SSL) strategies, and (iii) focused tuning of the best SSL method. All experiments were run as HPC array jobs for parallelism and resource isolation. Training and evaluation metrics were logged to Weights \& Biases (W\&B), and run histories were aggregated via the W\&B Python API into Pandas DataFrames to generate figures. All hyperparameters and design choices (architecture, learning rate, weight decay, consistency weight, etc.) were selected based solely on validation performance - the test split remained untouched until a final evaluation of the selected configuration.

\subsection{Stage 1: Architecture and Hyperparameter Search}

\textbf{Setup:} We first identified a suitable backbone for QM9 regression under a fully supervised setting. We compared four architectures: GCN, GIN, SchNet, and DimeNet++. For each model we ran a grid search over two learning rates ($5\times10^{-4}$ and $1\times10^{-3}$) and three weight decay values ($0$, $1\times10^{-4}$, $5\times10^{-4}$), yielding $4 \times 2 \times 3 = 24$ configurations. Before this grid search, we performed small exploratory runs to choose reasonable depths and optimizers. For simple MPNNs such as GCN, increasing depth beyond 4 - 5 layers led to performance degradation, consistent with over-smoothing. In contrast, geometric models such as SchNet were sensitive to large learning rates and required more conservative optimization settings.

\textbf{System optimization:} Profiling on the HPC cluster revealed a bottleneck in CPU-GPU data transfer and DataLoader worker initialization. By tuning the \texttt{num\_workers} parameter to match available CPU cores and increasing the (inference) batch size to use GPU memory more fully, we reduced the wall-clock time per epoch by roughly a factor of five. This speedup was crucial for running the larger sweeps in later stages.

\textbf{Results:} Figure~\ref{fig:stage1_combined} summarizes the Stage~1 results. DimeNet++ achieved the lowest median validation MSE (approx. 0.018) and showed the most robust performance across the hyperparameter grid, clearly outperforming GCN and GIN and slightly outperforming SchNet. This matches the expectation that QM9 targets depend strongly on 3D geometry, which DimeNet++ captures via directional message passing. Validation learning curves (Figure~\ref{fig:stage1_combined}b) also show that while higher learning rates can accelerate initial descent, they introduce instability for the more expressive geometric models.

Based on the best validation MSE in this stage, we selected DimeNet++ as the backbone for all subsequent experiments. The best supervised configuration from this search (learning rate $1\times10^{-3}$, weight decay $5\times10^{-4}$) was used as the starting point in Stage~2.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.225\textwidth}
        \centering
        \includegraphics[width=\linewidth]{stage1_boxplot.png}
        \caption{Architecture performance.}
        \label{fig:stage1_boxplot}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.225\textwidth}
        \centering
        \includegraphics[width=\linewidth]{stage1_learning_curves.png}
        \caption{Validation learning curves.}
        \label{fig:stage1_curves}
    \end{subfigure}
    \caption{\textbf{Stage~1: supervised architecture search on QM9.} (a) Distribution of best validation MSE across learning rate and weight decay settings for each architecture. DimeNet++ exhibits the lowest median error. (b) Validation MSE trajectories for selected runs, illustrating faster and more stable convergence for DimeNet++.}
    \label{fig:stage1_combined}
\end{figure}

\subsection{Stage 2: Semi-Supervised Strategy Comparison}

\textbf{Setup:}
With DimeNet++ fixed as the backbone, Stage~2 compared different SSL strategies under a constant labeled and unlabeled split. We evaluated: a multi-network consistency variant inspired by n-CPS (s-n-CPS), consistency regularization with geometric coordinate augmentations, and a Mean Teacher model with an exponential moving average (EMA) teacher. All methods used the same optimizer settings as the best supervised configuration from Stage~1 (learning rate $1\times10^{-3}$, weight decay $5\times10^{-4}$), as well as the same labeled/unlabeled/validation splits, to isolate the effect of the SSL methodology.

\textbf{Results:} Figure~\ref{fig:stage2_curves} shows the validation MSE trajectories. The Mean Teacher approach displays the most stable convergence and reaches the lowest validation error in this stage (final validation MSE around $0.0165$). Both s-n-CPS and the augmentation-based consistency model exhibit higher variance and somewhat noisy validation curves, which we attribute to the stochasticity of pseudo-labeling and strong geometric perturbations. The fully supervised baseline converges reliably but remains above the Mean Teacher curve. These results led us to select Mean Teacher as the SSL strategy to focus on in Stage~3.


\begin{figure}[t]
    \centering
    \begin{minipage}[c]{0.58\linewidth}
        \includegraphics[width=\linewidth]{stage2_wandb.png}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.38\linewidth}
        \caption{\textbf{Stage~2: validation MSE for different SSL strategies with a DimeNet++ backbone.} Mean Teacher (magenta) converges fastest and achieves the lowest final validation error ($\approx 0.0165$), while alternative SSL variants are less stable.}
        \label{fig:stage2_curves}
    \end{minipage}
\end{figure}

\subsection{Stage 3: Tuning and Regularizing Mean Teacher}

\textbf{Setup and challenges:} Stage~3 focused on improving the Mean Teacher configuration identified in Stage~2. Initial sweeps revealed a characteristic overfitting pattern: the supervised training loss approached zero, while validation MSE reached a minimum relatively early and then degraded. We traced this behavior to a combination of high effective learning rate and the large capacity of the standard DimeNet++ configuration combined with too long training, which allowed the student to effectively memorize the labeled data and reduce the impact of the unsupervised signal.

\textbf{Final configuration:} To mitigate these issues, we systematically varied four hyperparameters: coordinate noise standard deviation, unsupervised consistency weight, learning-rate decay factor, and weight decay. We then adopted a configuration with a monotonic learning-rate decay schedule (StepLR with $\gamma = 0.999$), stronger weight decay ($5\times10^{-3}$), an unsupervised loss weight of $1.0$, moderate coordinate noise, and a shorter training horizon of 50 epochs. This combination reduced the gap between training and validation losses and produced substantially more stable Mean Teacher training.

The best run from this stage achieved a validation MSE of $\mathbf{0.0124}$ (RMSE $\approx 0.111$ eV, MAE $\approx 0.088$ eV) and a test MSE of $\mathbf{0.0133}$ (RMSE $\approx 0.115$ eV, MAE $\approx 0.090$ eV) when evaluated with the EMA teacher. These are the lowest errors observed across all stages, as you can see in Table~\ref{tab:ssl_comparison}, and are reported as our final results.

\begin{table}[h]
    \centering
    \caption{Validation MSE for the each of the three-stage protocol.}
    \label{tab:ssl_comparison}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Stage} & \textbf{Method} & \textbf{Val MSE} \\
        \midrule
        Stage 1 & Supervised DimeNet++ & 0.0180  \\
        Stage 2 & Mean Teacher (EMA)   & 0.0165    \\
        Stage 3 & Mean Teacher (EMA) optimized   & 0.0124  \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Performance Analysis}

Overall, the experiments show that:

\textbf{Backbone choice matters}. DimeNet++ clearly outperforms simpler topological GNNs on QM9, confirming that 3D geometric information is crucial for this task.

\textbf{SSL gains are modest but consistent}. Mean Teacher consistently improves over a purely supervised DimeNet++ baseline in the same low-label regime, whereas more aggressive pseudo-labeling schemes (s-n-CPS, strong augmentations) are unstable under our settings.

\textbf{Regularization is key}. For a high-capacity geometric model, achieving benefits from SSL requires careful regularization and scheduling - otherwise the supervised loss can dominate and the unlabeled data contributes little.

In short, our three-stage protocol provides a controlled comparison of SSL strategies in a label-scarce setting and demonstrates that a carefully tuned Mean Teacher can yield measurable improvements on top of a strong DimeNet++ backbone.

% In conclusion, while the fully supervised DimeNet++ baseline established a robust foundation (Val MSE: 0.0173), the Mean Teacher framework provided measurable gains in predictive precision. Our optimization pipeline successfully drove the error down from an initial SSL benchmark of 0.0165 to a final Validation MSE of \textbf{0.0124} (Test MSE: 0.0134), representing a substantial improvement over the supervised starting point.

% \begin{table}[h!]
%   \caption{Validation MSE for different models and optimizations. Lower is better.}
%   \label{tab:val_mse}
%   \centering
%   \begin{tabular}{@{}lc@{}}   % @{} removes extra left/right padding (ACM style)
%     \toprule
%     \textbf{Optimization / Change} & \textbf{Val MSE} $\downarrow$ \\
%     \midrule

%     \multicolumn{2}{@{}l}{\textbf{Base Models}} \\
%     GCN (baseline)                     & 0.0123 \\
%     GAT (baseline)                     & 0.0118 \\
%     SchNet (baseline)                  & 0.0111 \\

%     \midrule
%     \multicolumn{2}{@{}l}{\textbf{Hyperparameter Optimizations}} \\
%     GCN (random search)                & 0.0107 \\
%     GAT (Bayesian optimization)        & 0.0103 \\
%     SchNet (extended tuning)           & 0.0098 \\

%     \midrule
%     \multicolumn{2}{@{}l}{\textbf{Final Best Score}} \\
%     SchNet (extended tuning)           & \textbf{0.0098} \\
%     \bottomrule
%   \end{tabular}
% \end{table}

\section{Conclusion}
\label{sec:conclusion}

In this project, we examined whether semi-supervised learning can improve molecular property prediction when only a limited set of labeled molecules is available. Using the QM9 dataset, we represented molecules as graphs and evaluated several message-passing architectures under a fully supervised training regime. The Stage~1 experiments showed that models encoding geometric information offer a clear advantage: DimeNet++ achieved the lowest validation MSE across the hyperparameter grid, indicating that explicit modeling of inter-atomic distances and angles is essential for quantum-chemical targets such as the HOMO energy.

After fixing DimeNet++ as the backbone, we compared three semi-supervised strategies in Stage~2. Although all methods matched the supervised baseline, the gains were modest, and the Mean Teacher framework provided the most stable improvements. The subsequent tuning in Stage~3 confirmed that appropriate scheduling and regularization were necessary to prevent overfitting and to make the consistency signal effective. Under the final configuration, Mean Teacher reached a validation MSE of 0.0124 (MAE $\approx 0.088$ eV) and a test MSE of 0.0133 (MAE $\approx 0.090$ eV), improving on the supervised baseline but without altering the overall performance. Even small reductions in error can affect candidate rankings in virtual screening, so these gains may still be useful.

These results suggest that, for QM9 with the given label budget, a strong geometric GNN such as DimeNet++ already extracts most of the available signal from the labeled subset, leaving limited headroom for additional improvements from unlabeled data. At the same time, the consistently positive effect of Mean Teacher indicates that temporal ensembling and consistency constraints are a robust way to regularize high-capacity GNNs in low-label regimes, even when the underlying supervised baseline is strong. Absolute performance remains lower than models trained on the full labeled QM9 set or larger equivariant architectures, which is expected under our reduced-label setting and focus on semi-supervised effects rather than full-data optimization.

There are several promising directions for future work. A natural extension is to move closer to truly label-scarce settings by reducing the proportion of labeled molecules and investigating how the relative benefit of semi-supervised learning scales with label availability. It would also be interesting to explore richer, chemistry-aware graph augmentations, uncertainty-aware pseudo-labeling strategies, and alternative consistency objectives, as well as to evaluate these methods on additional molecular benchmarks and target properties. Finally, connecting the observed error reductions more directly to downstream decision metrics in virtual screening or lead optimization would provide a stronger link between semi-supervised molecular property prediction and practical drug-discovery pipelines.

\newpage

% \section{Formatting your paper}
% \label{sec:format}

% All printed material, including text, illustrations, and charts, must be kept within a print area of 7 inches (178 mm) wide by 9 inches (229 mm) high. Do not write or print anything outside the print area. The top margin must be 1 inch (25 mm), except for the title page, and the left margin must be 0.75 inch (19 mm). All \emph{text} must be in a two-column format. Columns are to be 3.39 inches (86 mm) wide, with a 0.24 inch (6 mm) space between them. Text must be fully justified.

% \section{Page title section}
% \label{sec:pagestyle}

% The paper title (on the first page) should begin 1.38 inches (35 mm) from the top edge of the page, centered, completely capitalized, and in Times 14-point, boldface type. The authors' name(s) and affiliation(s) appear below the title in capital and lower case letters. Papers with multiple authors and affiliations may require two or more lines for this information.

% \section{Type-style and fonts}
% \label{sec:typestyle}

% We strongly encourage you to use Times-Roman font. In addition, this will give the proceedings a more uniform look. Use a font that is no smaller than ten point type throughout the paper, including figure captions.

% This is a minimum spacing; 2.75 lines/cm (7 lines/inch) will make the paper much more readable. Larger type sizes require correspondingly larger vertical spacing. Please do not double-space your paper. True-Type 1 fonts are preferred.

% The first paragraph in each section should not be indented, but all the following paragraphs within the section should be indented as these paragraphs demonstrate.

% \section{Major headings}
% \label{sec:majhead}

% Major headings, for example, "1. Introduction", should appear in all capital letters, bold face if possible, centered in the column, with one blank line before, and one blank line after. Use a period (".") after the heading number, not a colon.

% \subsection{Subheadings}
% \label{ssec:subhead}

% Subheadings should appear in lower case (initial word capitalized) in boldface. They should start at the left margin on a separate line.

% \subsubsection{Sub-subheadings}
% \label{sssec:subsubhead}

% Sub-subheadings, as in this paragraph, are discouraged. However, if you must use them, they should appear in lower case (initial word capitalized) and start at the left margin on a separate line, with paragraph text beginning on the following line. They should be in italics.

% \section{Printing your paper}
% \label{sec:print}

% If the last page of your paper is only partially filled, arrange the columns so that they are evenly balanced if possible, rather than having one long column.

% In \LaTeX, to start a new column (but not a new page) and help balance the last-page column lengths, you can use the command \verb|\pagebreak| as demonstrated on this page (see the \LaTeX source below).

% \section{Illustrations, graphs, and photographs}
% \label{sec:illust}

% Illustrations must appear within the designated margins. They may span the two columns. If possible, position illustrations at the top of columns, rather than in the middle or at the bottom. Caption and number every illustration.

% Since there are many ways, often incompatible, of including images in a \LaTeX document, below is an example of how to do this.

% \begin{figure}[htb]
% \fbox{\rule{0pt}{4cm}\rule{0.97\columnwidth}{0pt}}
% \caption{Example of placing a figure with experimental results.}
% \label{fig:res}
% \end{figure}

% % To start a new column (but not a new page) and help balance the last-page
% % column length use \vfill\pagebreak.
% % -------------------------------------------------------------------------
% \vfill
% \pagebreak

% \section{Footnotes}
% \label{sec:foot}

% Use footnotes sparingly (or not at all!) and place them at the bottom of the column on the page on which they are referenced. Use Times 9-point type, single-spaced. To help your readers, avoid using footnotes altogether and include necessary peripheral observations in the text (within parentheses, if you prefer, as in this sentence).

% \section{References}
% \label{sec:ref}

% List and number all bibliographical references at the end of the paper. References may be numbered (either alphabetically or in order of appearance) or follow the author–year citation style (e.g., using the \texttt{natbib} package). If you use a numeric style, cite references using square brackets, e.g., \cite{C2}. If you use an author–year style, cite using round brackets.

\bibliographystyle{IEEEbib}
\bibliography{references}

\section*{GitHub Repository}
\textbf{Link to GitHub repository:} \url{https://github.com/victorwinther/Semi-supervised-learning-for-drug-discovery}

\clearpage
\newpage

\section*{Declaration of use of generative AI}
This declaration \textbf{must} be filled out and included as the \textbf{final page} of the document. The questions apply to all parts of the work, including research, project writing, and coding.
\\\\
\textbf{We have used generative AI tools:} Yes
\\\\
\textbf{List of generative AI tools used:}
\begin{itemize}
    \item ChatGPT (OpenAI)
    % \item GitHub copilot?
    \item Gemini (Google)
    \item Claude (Anthropic)
\end{itemize}
\textbf{Tool use:} We used LLMs during the ideation phase, for clarifying theoretical concepts, proof-reading of sections we had drafted ourselves, and debugging for code snippets and LaTeX formatting.
\\\\
\textbf{Stages of tool use:} We used the tool during the project planning and literature review phase (to clarify methods and terminology), during implementation/debugging, and during report writing for wording suggestions and LaTeX help.
\\\\
\textbf{Use of generated output:} All generated output was treated as suggestions. We reviewed, edited, and integrated text manually,  and we verified all technical content, equations, and references ourselves. Any code suggested by the tool was adapted, integrated, and tested by us. The final design of experiments, analysis of results, and conclusions are our own work.

\end{document}